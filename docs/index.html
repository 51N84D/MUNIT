<!doctype html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title></title><link href='https://fonts.loli.net/css?family=Merriweather:900,900italic,300,300italic&subset=latin-ext' rel='stylesheet' type='text/css' />
<link href='https://fonts.loli.net/css?family=Lato:900,300&subset=latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }


:root { --control-text-color: #777; }
h1, .h1, .f1 { font-size: 2rem; line-height: 2.5rem; }
h2, .h2, .f2 { font-size: 1.5rem; line-height: 2rem; }
h3, .h3, .f3 { font-size: 1.25rem; line-height: 1.5rem; }
p, .p, .f4, h4, h5, h6, dl, ol, ul, pre[cid], div[cid], #typora-source { font-size: 1.125rem; line-height: 1.5rem; }
h4 { font-size: 1.13rem; }
body { font-family: Merriweather, "PT Serif", Georgia, "Times New Roman", STSong, serif; line-height: 1.5rem; font-weight: 400; }
#write { max-width: 914px; color: rgb(51, 51, 51); }
img { width: auto; max-width: 100%; }
body { font-size: 1.5rem; box-sizing: border-box; }
.ty-table-edit { background: rgb(237, 237, 237); }
table { width: 100%; font-size: 1.125rem; }
table > thead > tr > th, table > thead > tr > td, table > tbody > tr > th, table > tbody > tr > td, table > tfoot > tr > th, table > tfoot > tr > td { padding: 12px; line-height: 1.2; vertical-align: top; border-top: 1px solid rgb(51, 51, 51); }
table > thead > tr > th { vertical-align: bottom; border-bottom: 2px solid rgb(51, 51, 51); }
table > caption + thead > tr:first-child > th, table > caption + thead > tr:first-child > td, table > colgroup + thead > tr:first-child > th, table > colgroup + thead > tr:first-child > td, table > thead:first-child > tr:first-child > th, table > thead:first-child > tr:first-child > td { border-top: 0px; }
table > tbody + tbody { border-top: 2px solid rgb(51, 51, 51); }
p { font-weight: 300; line-height: 1.5; }
abbr { border-bottom: 1px dotted black; cursor: help; }
pre, code { font-family: Menlo, Monaco, "Courier New", monospace; }
code, .md-fences { color: rgb(122, 122, 122); }
.md-fences { padding: 0.5rem 1.125em; margin-bottom: 0.88em; font-size: 1rem; border: 1px solid rgb(122, 122, 122); }
blockquote { padding: 1.33em; font-style: italic; border-left: 5px solid rgb(122, 122, 122); color: rgb(85, 85, 85); }
blockquote em { color: rgb(0, 0, 0); }
blockquote footer { font-size: 0.85rem; font-style: normal; background-color: rgb(255, 255, 255); color: rgb(122, 122, 122); border-color: transparent; }
h1, .h1, h2, .h2, h3, .h3, h4, .h4, h5, .h5, h6, .h6 { font-family: Lato, "Helvetica Neue", Helvetica, sans-serif; font-weight: bold; line-height: 1.2; margin: 1em 0px 0.5em; }
@media screen and (min-width: 48em) {
  .h1, h1 { font-size: 3.25rem; }
  .h2, h2 { font-size: 2.298rem; }
  .h3, h3 { font-size: 1.625rem; }
  .h4, h4 { font-size: 1.3rem; }
  #write > h4.md-focus::before, #write > h5.md-focus::before, #write > h6.md-focus::before { top: 1px; }
  .p, p, li { font-size: 1.25rem; line-height: 1.8; }
  table { font-size: 1.25rem; }
}
@media (max-width: 48em) {
  blockquote { margin-left: 1rem; margin-right: 0px; padding: 0.5em; }
  .h1, h1 { font-size: 2.827rem; }
  .h2, h2 { font-size: 1.999rem; }
  .h3, h3 { font-size: 1.413rem; }
  .h4, h4 { font-size: 1.3rem; }
}
@media screen and (min-width: 64em) {
  .h1, h1 { font-size: 4.498rem; }
  .h2, h2 { font-size: 2.29rem; }
  .h3, h3 { font-size: 1.9rem; }
  .h4, h4 { font-size: 1.591rem; }
  #write > h4.md-focus::before { top: 4px; }
}
a { color: rgb(70, 63, 92); text-decoration: underline; }
#write { padding-top: 2rem; }
#write pre.md-meta-block { min-height: 35px; padding: 2000px 1em 10px 0px; white-space: pre; border-width: 0px 30px; border-top-style: initial; border-bottom-style: initial; border-top-color: initial; border-bottom-color: initial; border-image: initial; border-left-style: solid; border-left-color: rgb(248, 248, 248); border-right-style: solid; border-right-color: rgb(248, 248, 248); width: 100vw; max-width: calc(100% + 60px); margin-left: -30px; margin-bottom: 2em; margin-top: -2010px; line-height: 1.5em; color: rgb(122, 122, 122); background-color: rgb(250, 250, 250); font-family: Lato, "Helvetica Neue", Helvetica, sans-serif; font-weight: 300; clear: both; font-size: 1.125rem; }
.md-image > .md-meta { color: rgb(70, 63, 92); }
.footnotes { font-size: 1.1rem; }
.md-tag { font-family: Lato, "Helvetica Neue", Helvetica, sans-serif; }
.code-tooltip { background: white; }
.code-tooltip-content { font-size: 1.1rem; }
.task-list { padding-left: 0px; }
.md-task-list-item { padding-left: 34px; }
.md-task-list-item > input { width: 1.25rem; height: 1.25rem; display: block; -webkit-appearance: initial; top: -0.2rem; margin-left: -1.6em; margin-top: calc(1rem - 7px); border: none; }
.md-task-list-item > input:focus { outline: none; box-shadow: none; }
.md-task-list-item > input::before { border: 1px solid rgb(85, 85, 85); border-radius: 1.5rem; width: 1.5rem; height: 1.5rem; background: rgb(255, 255, 255); content: " "; transition: background-color 200ms ease-in-out 0s; display: block; }
.md-task-list-item > input:checked::before, .md-task-list-item > input[checked]::before { background: rgb(51, 51, 51); border-width: 2px; display: inline-block; transition: background-color 200ms ease-in-out 0s; }
.md-task-list-item > input:checked::after, .md-task-list-item > input[checked]::after { opacity: 1; }
.md-task-list-item > input::after { transition: opacity 0.05s ease-in-out 0s; transform: rotate(-45deg); position: absolute; top: 0.4375rem; left: 0.28125rem; width: 0.9375rem; height: 0.5rem; border-width: 0px 0px 3px 3px; border-bottom-style: solid; border-left-style: solid; border-bottom-color: rgb(255, 255, 255); border-left-color: rgb(255, 255, 255); border-image: initial; border-top-style: initial; border-top-color: initial; border-right-style: initial; border-right-color: initial; content: " "; opacity: 0; }
.md-tag { color: inherit; }
.md-toc:focus .md-toc-content { margin-top: 19px; }
#typora-sidebar { font-size: 1rem !important; }
.html-for-mac #typora-sidebar { background-color: white; }
.outline-content li, .outline-content ul { font-size: 1rem !important; }
.outline-title { line-height: inherit; margin-top: 10px; }
.outline-expander { width: 18px; }
.outline-expander::before { content: "+"; font-family: inherit; color: rgb(108, 108, 108); font-size: 1.5rem; top: 0.1rem; }
.outline-expander:hover::before { content: "+"; }
.outline-item-open > .outline-item > .outline-expander::before { content: "-"; }
#typora-source { font-family: Courier, monospace; color: rgb(106, 106, 106); }
.os-windows #typora-source { font-family: inherit; }
.cm-s-typora-default .cm-header, .cm-s-typora-default .cm-property, .CodeMirror.cm-s-typora-default div.CodeMirror-cursor { color: rgb(66, 139, 202); }
.cm-s-typora-default .cm-atom, .cm-s-typora-default .cm-number { color: rgb(119, 119, 119); }
.md-diagram-panel { margin-top: 24px; margin-left: -1.2em; }
.md-mathjax-midline { background: rgb(250, 250, 250); }
.enable-diagrams pre.md-fences[lang="sequence"] .code-tooltip, .enable-diagrams pre.md-fences[lang="flow"] .code-tooltip, .enable-diagrams pre.md-fences[lang="mermaid"] .code-tooltip { bottom: -3.4em; }
.dropdown-menu .divider { border-color: rgb(229, 229, 229); }


</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><p><a href='#introduction'><span>Introduction</span></a></p><p><a href='#technical_proposal'><span>Technical Proposal</span></a></p><ul><li><a href='#cycleGAN'><span>CycleGAN</span></a></li><li><a href='#instaGAN'><span>InstaGAN</span></a></li><li><a href='#generative-image-inpainting'><span>Generative Image Inpainting</span></a></li></ul><p><a href='#current_approach'><span>Current Approach</span></a></p><ul><li><a href='#simulated_data'><span>Simulated Data</span></a></li></ul><p><a href='#height_estimation'><span>Height estimation</span></a></p><ul><li><a href='#geometrical_approaches'><span>Geometrical approaches</span></a></li><li><a href='#end-to-end-height-estimator'><span>End-to-End height estimator</span></a></li></ul><p>&nbsp;</p></div>
</body>
<div  id='write'  class = 'is-node'><h1><a name="introduction" class="md-header-anchor"></a><span>Introduction</span></h1><p style="text-align:justify;">
    Dramatic and rapid changes to the global economy are required in order to limit climate-related risks for natural and human systems (IPCC, 2018). <b>Governmental interventions are needed to fight climate change</b> and they need strong public support.  However,  <b>it is difficult to mentally simulate the complex effects of climate change</b> (O’Neill &amp; Hulme, 2009) and people often discount the impact that their actions will have on the future, especially if the <b>consequences are long-term</b>, <b>abstract</b>, and at odds with current behaviors and identities (Marshall, 2015).
    </p> <ul><li><span>We are developing a tool to </span><strong><span>help the public understand the consequences of climate change.</span></strong><span> </span></li><li><span>We intend to </span><strong><span>make people aware of Climate Change</span></strong><span> </span><strong><span>in their direct environment</span></strong><span> by showing them concrete examples. </span></li></ul>
        

<p style="text-align:justify;">Currently we are focusing on simulating images of one specific extreme climate event: floods. We are aiming to create a flood simulator which, given a user-entered address, is able to extract a street view image of the surroundings and to alter it to generate a plausible image projecting flood where it is likely to occur.</p>

<div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/flooding2.gif" style="zoom:100%;" alt="{{ include.description }}" class="center" scrolling="no"> 
  <figcaption>Visualization made with a Generative Adversarial Network (GAN) </figcaption> 
</figure>
</div>

<p style="text-align:justify;">Recent research has explored the <b>potential of translating numerical climate models into representations</b>  that are intuitive and easy to understand, for instance via <b>climate-analog mapping</b>  (Fitzpatrick et al., 2019) and by leveraging relevant social group norms (van der Linden, 2015). Other approaches have focused on selecting relevant images to best represent climate change impacts (Sheppard, 2012; Corner &amp; Clarke, 2016) as well as using artistic renderings of possible future landscapes (Giannachi, 2012) and even video games (Angel et al., 2015). However, to our knowledge, our project is the <b>first application of generative models to generate images of future climate change scenarios.</b>  </p><h1><a name="technical-proposal" class="md-header-anchor"></a><span>Technical Proposal</span></h1><p style="text-align:justify;">We propose to use <b>Style Transfer</b> and especially Unsupervised <b> Image To Image Translation techniques</b> to learn a transformation from a natural image of a house to its flooded version.  This technology can leverage the quantity of cheap-to-acquire unannotated images.  </p><blockquote><p><strong><span>Image To Image Translation</span></strong>
<span>:   A class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs</span></p><p><strong><span>Style Transfer</span></strong>
<span>:   Aims to modify the style of an image while preserving its content.</span>
<a href='https://junyanz.github.io/CycleGAN/'><span>																																	</<span><span>CycleGAN (Zhu et al., 2017)</span></a><span> </span></p></blockquote><p style="text-align:justify;">Let <a href="https://www.codecogs.com/eqnedit.php?latex=x_1&amp;space;\in&amp;space;X_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_1&amp;space;\in&amp;space;X_1" title="x_1 \in X_1"></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=x_2&amp;space;\in&amp;space;X_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_2&amp;space;\in&amp;space;X_2" title="x_2 \in X_2"></a>  be images from two different image domains. <a href="https://www.codecogs.com/eqnedit.php?latex=X_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_1" title="X_1"></a> represents the  non-flooded domain which gathers several type of street-level imagery defined later in the data section and <a href="https://www.codecogs.com/eqnedit.php?latex=X_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_2" title="X_2"></a> is the flooded domain composed of images where a part of a single house or building is visible and the street is partially or fully covered by water. </p>
<div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_1.png" style="zoom:38%;" alt="{{ include.description }}" class="center"> 
  <figcaption><b>Non-flood sample:</b>&nbsp;&nbsp;&nbsp;<a href="https://www.codecogs.com/eqnedit.php?latex=x_1&amp;space;\in&amp;space;X_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_1&amp;space;\in&amp;space;X_1" title="x_1 \in X_1"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <b>Flood sample:</b>&nbsp;&nbsp;&nbsp;<a href="https://www.codecogs.com/eqnedit.php?latex=x_2&amp;space;\in&amp;space;X_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_2&amp;space;\in&amp;space;X_2" title="x_2 \in X_2"></a> </figcaption> 
</figure>
</div><p style="text-align:justify;">In the unpaired image-to-image translation setting, we are given samples drawn from two marginal distributions : <img src="https://latex.codecogs.com/gif.latex?$x_1&amp;space;\sim&amp;space;p(x_1)$" title="$x_1 \sim p(x_1)$">samples of (non-flooded) houses and <img src="https://latex.codecogs.com/gif.latex?$x_2&amp;space;\sim&amp;space;p(x_2)$" title="$x_2 \sim p(x_2)$"> samples of flooded houses, without access to the joint distribution <img src="https://latex.codecogs.com/gif.latex?$p(x_1,x_2)$" title="$p(x_1,x_2)$">.</p>
<p style="text-align:justify;">From a probability theory viewpoint, the key challenge is to learn the joint distribution while only observing the marginals. Unfortunately, there is an infinite set of joint distributions that correspond to the given marginal distributions <a href="https://en.wikipedia.org/wiki/Coupling_(probability)">(cf coupling theory)</a>. Inferring the joint distribution from the marginals is a highly ill-defined problem. Assumptions are required to constrain the structure of the joint distribution, such as those introduced by the authors of CycleGAN. </p>
<p style="text-align:justify;">In our case we are estimating the complex conditional distribution <img src="https://latex.codecogs.com/gif.latex?$p(x_2|x_1)$" title="$p(x_2|x_1)$"> with different image-to-image translation models <img src="https://latex.codecogs.com/gif.latex?$p(x_{1\rightarrow&amp;space;2}|x_1)$" title="$p(x_{1\rightarrow 2}|x_1)$">, where <img src="https://latex.codecogs.com/gif.latex?$x_{1\rightarrow&amp;space;2}$" title="$x_{1\rightarrow 2}$"> is a sample produced by translating <img src="https://latex.codecogs.com/gif.latex?$x_1$" title="$x_1$"> to <img src="https://latex.codecogs.com/gif.latex?$X_2$" title="$X_2$">. </p><h2><a name="cyclegan" class="md-header-anchor"></a><span>CycleGAN </span></h2><p style="text-align:justify;">CycleGAN is one of the research papers that revolutionized image-to-image translation in an unpaired setting. It has been used as the first proof of concept for this project. </p>
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_2.png" style="zoom:40%;" alt="{{ include.description }}" class="center"> 
  <figcaption> Results of the proof of concept from <a href="https://arxiv.org/abs/1905.03709">Schmidt et al., 2019</a> </figcaption> 
</figure>
<p><span>It aims to capture the style from one image collection and to learn how to apply it to the other image collection. There are two main constraints in order to ensure conversion and coherent transformation:</span></p><p style="text-align:justify;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The <b>indistinguishable</b> constraint: The produced output has to be indistinguishable from the samples of the new domain. This is enforced using the GAN Loss <a href="https://arxiv.org/abs/1406.2661">Goodfellow et al., 2014</a> and is applied at the distribution level. In our case, the mapping of the non-flood domain to the flood domain should create images that are indistinguishable from the training images of floods and vice-versa <a href="https://arxiv.org/abs/1709.00074">Galenti et al., 2017</a>. But this constraint alone is not enough to map an input image in domain <a href="https://www.codecogs.com/eqnedit.php?latex=X_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_1" title="X_1"></a> to an output image  <a href="https://www.codecogs.com/eqnedit.php?latex=X_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_2" title="X_2"></a>  with the same semantics. The network could learn to generate realistic images from domain  <a href="https://www.codecogs.com/eqnedit.php?latex=X_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_2" title="X_2"></a> without preserving the content of the input image. This latter point is tackled by the cycle consistency constraint.</p> 
<p style="text-align:justify;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The <b> Cycle-Consistency Constraint</b>  aims to regularize the mapping of the two domains in a <i> meaningful way </i>. It can be described as imposing a structural constraint which states that if we translate from one domain to the other and back again we should arrive at where we started. Formally, if we have a translator <img src="https://latex.codecogs.com/gif.latex?$G:X_1&amp;space;\rightarrow&amp;space;X_2$" title="$G:X_1 \rightarrow X_2$"> and another translator <img src="https://latex.codecogs.com/gif.latex?$F:X_2&amp;space;\rightarrow&amp;space;X_1$" title="$F:X_2 \rightarrow X_1$"> then <img src="https://latex.codecogs.com/gif.latex?$G$" title="$G$"> and <img src="https://latex.codecogs.com/gif.latex?$F$" title="$F$">should be bijections and inverses of each other. </p>

<div style="text-align: center">
  <figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/scheme_cycle_consistency.png" style="zoom:55%;" alt="{{ include.description }}" class="center"> 
  <figcaption> </figcaption>
</figure>
</div>
<ul><li><p style="text-align:justify;">(a) The CycleGAN model contains two mapping functions <img src="https://latex.codecogs.com/gif.latex?G:X_1&amp;space;\rightarrow&amp;space;X_2" title="G:X_1 \rightarrow X_2"> and <img src="https://latex.codecogs.com/gif.latex?F:X_2&amp;space;\rightarrow&amp;space;X_1" title="$F:X_2 \rightarrow X_1$"> , and associated adversarial discriminators <img src="https://latex.codecogs.com/gif.latex?$D_{X_2}$" title="$D_{X_2}$"> and <img src="https://latex.codecogs.com/gif.latex?$D_{X_1}$" title="$D_{X_1}$">. <img src="https://latex.codecogs.com/gif.latex?$D_{X_2}$" title="$D_{X_2}$"> encourages <img src="https://latex.codecogs.com/gif.latex?$G$" title="$G$"> to translate <img src="https://latex.codecogs.com/gif.latex?X_1" title="X_1"> into outputs indistinguishable from domain <img src="https://latex.codecogs.com/gif.latex?X_2" title="$X_2$">, and vice versa for <img src="https://latex.codecogs.com/gif.latex?$D_{X_1}$" title="$D_{X_1}$"> and <img src="https://latex.codecogs.com/gif.latex?$F$" title="$F$">. </p></li><li><p style="text-align:justify;">(b) Forward cycle-consistency loss: <img src="https://latex.codecogs.com/gif.latex?$x_1\rightarrow&amp;space;G(x_1)&amp;space;\rightarrow&amp;space;F(G(x_1))&amp;space;\approx&amp;space;x_1$" title="$x_1\rightarrow G(x_1) \rightarrow F(G(x_1)) \approx x_1$"> </p></li><li><p style="text-align:justify;">(c) Backward cycle-consistency loss: <img src="https://latex.codecogs.com/gif.latex?$x_2&amp;space;\rightarrow&amp;space;F(x_2)&amp;space;\rightarrow&amp;space;G(F(x_2))&amp;space;\approx&amp;space;x_2&amp;space;$" title="$x_2 \rightarrow F(x_2) \rightarrow G(F(x_2)) \approx x_2 $"> </p></li></ul><p style="text-align:justify;"><b>Pros and cons:</b> The most advantageous part of this approach is its total lack of supervision, which means that the access to data is cheap (1K images of non-flooded and flooded houses). The major problem is that the style transfer is applied to the entire image.</p>
<p style="text-align:justify;"><b>Initial Results:</b> When the ground is not concrete but grass and vegetation, CycleGAN generates a brown flood of low quality with blur on the edges between houses and grass. The color of the sky changes from blue to grey (probably because of the bias on the training set of flood images). </p><h2><a name="instagan" class="md-header-anchor"></a><span>InstaGAN</span></h2><p style="text-align:justify;">The <a href="https://arxiv.org/abs/1406.2661">InstaGAN</a> architecture is built on the foundations of CycleGAN. The main idea of their approach is to incorporate instance attributes <img src="https://latex.codecogs.com/gif.latex?\mathcal{A}" title="\mathcal{A}"> (and <img src="https://latex.codecogs.com/gif.latex?\mathcal{B}" title="\mathcal{B}">) to the source <img src="https://latex.codecogs.com/gif.latex?X_1" title="X_1"> (and the target <img src="https://latex.codecogs.com/gif.latex?X_2" title="X_2">) domain to improve the image-to-image translation. They describe their approach as learning joint mappings between attribute-augmented spaces <img src="https://latex.codecogs.com/gif.latex?X_1" title="X_1"> × <img src="https://latex.codecogs.com/gif.latex?\mathcal{A}" title="\mathcal{A}"> and <img src="https://latex.codecogs.com/gif.latex?X_2" title="X_2"> × <img src="https://latex.codecogs.com/gif.latex?\mathcal{B}" title="\mathcal{B}">. </p>
<p style="text-align: center">
</p>
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_3.png" style="zoom:55%;" alt="{{ include.description }}" class="center"> 
  <figcaption>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Overview of the Network &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Generator &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Discriminator &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</figcaption> <p></p></figure><p style="text-align:justify;">In our setting, the set of instance attributes <img src="https://latex.codecogs.com/gif.latex?a\in\mathcal{A}" title="a\in\mathcal{A}">  is reduced to one attribute: a segmentation mask of <b>where-to-flood</b>  and for the attribute of <img src="https://latex.codecogs.com/gif.latex?b\in\mathcal{B}" title="b\in\mathcal{B}"> a segmentation mask covering the <b>flood</b>. Each network is designed to encode both an image and a set of masks (in our case a single mask). </p>
<p style="text-align:justify;">The authors explicitly say that any useful information could be incorporated as an attribute and claim that their approach leads to disentangle different instances within the image allowing the generator to perform <b>accurate</b> and <b>detailed</b> translation from one instance to the other. </p>
<div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_instagan.png" style="zoom:55%;" alt="{{ include.description }}" class="center"> 
  <figcaption>Best-case scenario: failure cases &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</figcaption> 
</figure>
</div><p style="text-align:justify;"><b>Pros and cons:</b>. As for CycleGAN InstaGAN doesn't need paired images but requires the knowledge of some attributes, here the masks. Sometimes the model is able to render water in a realistic manner, including reflections and texture. But a major drawback is that, although it's penalized during training, it continues to modify the rest of the image (the unmasked region): colors change, artifacts appear, textures are different and fine details are blurred. </p>
<p style="text-align:justify;"> <b>Results:</b> Empirically we find that it works well with grass but not with concrete. Transparency is a big issue with InstaGAN's results on our task, since most of the time we can see the road lanes through the flood. Even in synthetic settings with aligned images InstaGAN generates relatively realistic water texture which remains transparent. We could conclude that it learns to reflect the sky on the water (whatever the color of the sky is), resulting in the fact that sometimes it paints blue on the concrete itself without the accompanying water texture. In our case results quality worsen dramatically out of the training set.</p>
<p style="text-align:justify;"><b>Note:</b> The instances used in the papers are either segmentation mask of animals (e.g. translating sheep to giraffe), or segmentation mask of clothes (e.g. translating pants to skirt). In both cases, I found that theses instances are <em>less diverse</em> than instances from our non-flood to flood translation in the sense that sheep <em> color, shape, texture </em> is less diverse than the examples of flood or street in our dataset. </p><h2><a name="generative-image-inpainting" class="md-header-anchor"></a><span>Generative Image Inpainting</span></h2><p style="text-align:justify;">Previous approaches based on modification of CycleGAN does not give us a fine control over the region that should be flooded. Assuming we are able to identify such a region in the image, we would only need to learn how to render water realistically. There are a lot of promising image edition techniques in the GAN literature demonstrating how to perform edition of specific attributes, morphing images or manipulating the semantic. These transformation are often performed on small latent space of generated fake images.  However, natural image edition is a lot harder and there is no easy way of manipulating the semantic of natural images. </p>
<p style="text-align:justify;"><b>Image Inpainting</b> is the technique of modifying and restoring a damaged image in a visually plausible way. Given recent advances in the field, it is now possible to <em>guess</em> lost information and replace it with plausible content at real-time speed. </p>
<div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_inpainting.png" style="zoom:70%;" alt="{{ include.description }}" class="center"> 
  <figcaption>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Destructed Image &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reconstructed Image with DeepFill</figcaption> 
</figure>
</div><p style="text-align:justify;">For example, a recent deep-generative model exploiting contextual attention: <a href="http://jiahuiyu.com/deepfill/">DeepFill</a>, is able to reconstruct high definition altered images of faces and landscapes at real-time speed. We believe that there is a way of leveraging the network generation capacity and apply its mechanisms to our case. Our experiment consist in biasing DeepFill to reconstruct only region where there is water (without surrounding water).  We trained the network with several hundreds images of flood where the water was replaced by a grey mask. At inference we replaced what we defined as the ground with a grey mask. (see results below) </p>
<div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_5.png" style="zoom:55%;" alt="{{ include.description }}" class="center"> 
  <figcaption>Deepfill biased to replace a given mask (here the ground) with plausible water. </figcaption> 
</figure>
</div><p style="text-align:justify;"><b>Results:</b> The quality of the result is bad when given large masks. This could be explain by the fact that the architecture is designed to extract information from the context in the image: in the former experiment the network had to <em>draw</em> from a context where water is inexistent. To pursue research in that direction, one may want to give a better context to the network by using example of water texture on the side of the image. Or by increasing the dataset of images where there is water. </p><h1><a name="current-approach" class="md-header-anchor"></a><span>Current Approach</span></h1><p style="text-align:justify;">Our current approach is built on <a href="https://arxiv.org/abs/1804.04732">MUNIT</a>. In the paper, a <em>partially shared latent space assumption</em> is made. It is assumed that images can be disentangled into a content code (domain-invariant) and a style code (domain-dependant). In this assumption, each image <img src="https://latex.codecogs.com/gif.latex?x_{i}\in\mathcal{X}_{i}" title="x_{i}\in\mathcal{X}_{i}"> is generated from a content latent code <img src="https://latex.codecogs.com/gif.latex?c\in&amp;space;\mathcal{C}" title="c\in \mathcal{C}"> that is shared by both domains, and a style latent code <img src="https://latex.codecogs.com/gif.latex?s_{i}\in&amp;space;\mathcal{S}_{i}" title="s_{i}\in \mathcal{S}_{i}"> that is specific to the individual domain. </p><div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_munit.png" style="zoom:45%;" alt="{{ include.description }}" class="center"> <figcaption>(a) Overview of MUNIT Autoencoder&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Off the shelf model trained on our dataset.</figcaption> 
</figure>
</div><p style="text-align:justify;">In other words, a pair of corresponding images <img src="https://latex.codecogs.com/gif.latex?(x_{1},x_{2})" title="(x_{1},x_{2})"> from the joint distribution is assumed to be generated by <img src="https://latex.codecogs.com/gif.latex?x_{1}&amp;space;=&amp;space;G^{*}_{1}(c,&amp;space;s_{1})" title="x_{1} = G^{*}_{1}(c, s_{1})"> and <img src="https://latex.codecogs.com/gif.latex?x_{2}&amp;space;=&amp;space;G^{*}_{2}(c,&amp;space;s_{2})" title="x_{2} = G^{*}_{2}(c, s_{2})">, where <img src="https://latex.codecogs.com/gif.latex?c,&amp;space;s_{1},&amp;space;s_{2}" title="c, s_{1}, s_{2}"> are from some prior distributions and <img src="https://latex.codecogs.com/gif.latex?G^{*}_{1}" title="G^{*}_{1}">, <img src="https://latex.codecogs.com/gif.latex?G^{*}_{2}" title="G^{*}_{2}"> are the underlying generators. Given the former hypothesis, the goal is to learn the underlying generator and encoder functions with neural networks.</p><div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_itit_munit.png" style="zoom:100%;" alt="{{ include.description }}" class="center">  <figcaption>MUNIT image-to-image translation model consists of two auto-encoders (denoted by <font color="red">red</font> and <font color="blue">blue</font> arrows respectively), one for each domain. The latent code of each auto-encoder is composed of a content code <img src="https://latex.codecogs.com/gif.latex?c" title="c"> and a style code <img src="https://latex.codecogs.com/gif.latex?s" title="s">.</figcaption> 
</figure>
</div><p style="text-align:justify;"><b>Image-to-image translation</b> is performed by swapping encoder-decoder pairs. For example, to translate a house  <img src="https://latex.codecogs.com/gif.latex?x_{1}\in&amp;space;\mathcal{X}_{1}" title="x_{1}\in \mathcal{X}_{1}">  to a flooded-house <img src="https://latex.codecogs.com/gif.latex?\mathcal{X}_{2}" title="\mathcal{X}_{2}">, one may use MUNIT to first extract the content latent code <img src="https://latex.codecogs.com/gif.latex?c_{1}&amp;space;=&amp;space;E^{c}_{1}(x_{1})" title="c_{1} = E^{c}_{1}(x_{1})"> of the house image that we want to flood and randomly draw a style latent code <img src="https://latex.codecogs.com/gif.latex?s_{2}" title="s_{2}"> from the prior distribution <img src="https://latex.codecogs.com/gif.latex?q(s_{2})\sim&amp;space;\mathcal{N}(0,&amp;space;\mathbf{I})" title="q(s_{2})\sim \mathcal{N}(0, \mathbf{I})"> of flooded-houses and then use <img src="https://latex.codecogs.com/gif.latex?G_{2}" title="G_{2}"> to produce the final output image <img src="https://latex.codecogs.com/gif.latex?x_{1\rightarrow&amp;space;2}&amp;space;=&amp;space;G_{2}(c_{1},&amp;space;s_{2})" title="x_{1\rightarrow 2} = G_{2}(c_{1}, s_{2})"> (content from <img src="https://latex.codecogs.com/gif.latex?x_1" title="x_1"> and style from <img src="https://latex.codecogs.com/gif.latex?x_2" title="x_2">). </p><h3><a name="how-does-it-work-" class="md-header-anchor"></a><span>How does it work ?</span></h3><p style="text-align:justify;"><a href="https://arxiv.org/abs/1703.06868">Huang et al.</a> demonstrated that Instance Normalization is deeply linked to style normalization. Munit transfer the style by modifying the features statistics, de-normalizing in a certain way.  Given an input batch <img src="https://latex.codecogs.com/gif.latex?x&amp;space;\in&amp;space;\mathbb{R}^{N\times&amp;space;C\times&amp;space;H\times&amp;space;W}" title="x \in \mathbb{R}^{N\times C\times H\times W}">, <b>Instance Normalization Layers</b> are used in MUNIT encoders to normalize feature statistics. </p>
<p style="text-align:justify;"><img src="https://latex.codecogs.com/gif.latex?IN(x)&amp;space;=&amp;space;\left&amp;space;(&amp;space;\frac{x-\mu(x)}{\sigma(x)}&amp;space;\right&amp;space;)" title="IN(x) = \gamma \left ( \frac{x-\mu(x)}{\sigma(x)} \right ) "> </p>
<p style="text-align:justify;">Where <img src="https://latex.codecogs.com/gif.latex?\mu(x)" title="\mu(x)"> and <img src="https://latex.codecogs.com/gif.latex?\sigma(x)" title="\sigma(x)">are computed as the mean and standard deviation across spatial dimensions independently for each channel and each sample. <b>Adaptative Instance normalization layers </b> are then used in the decoder to de-normalize the features statistics.  </p> 
<p style="text-align:justify;"><img src="https://latex.codecogs.com/gif.latex?AdaIN(z,\gamma,\beta)&amp;space;=&amp;space;\gamma&amp;space;\left&amp;space;(&amp;space;\frac{z-\mu(z)}{\sigma(z)}&amp;space;\right&amp;space;)&amp;space;+&amp;space;\beta" title="AdaIN(z,\gamma,\beta) = \gamma \left ( \frac{z-\mu(z)}{\sigma(z)} \right ) + \beta"> </p>
<p style="text-align:justify;">With <img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta"> and  <img src="https://latex.codecogs.com/gif.latex?\gamma" title="\gamma">  defined as a multi-layer perceptron (MLP), i.e., [<img src="https://latex.codecogs.com/gif.latex?\beta;\gamma" title="\beta;\gamma">] = [<img src="https://latex.codecogs.com/gif.latex?\beta(s);\gamma(s)" title="\beta(s);\gamma(s)">]=MLP<img src="https://latex.codecogs.com/gif.latex?(s)" title="(s)"> with  <img src="https://latex.codecogs.com/gif.latex?s" title="s"> the style. The fact that the de-normalization parameters are inferred with a MLP allow users to generate multiple output from one image. </p><h3><a name="modifying-the-network-to-fit-our-purpose" class="md-header-anchor"></a><span>Modifying the network to fit our purpose:</span></h3><p><span>We questioned and transformed the official MUNIT architecture to fit our purpose. </span></p><ul><li><p style="text-align:justify;">Because we wanted control over the translation, we removed randomness from the style: the network is then trained to perform style transfer with the style extracted from one image and not sampled from a normal distribution. </p></li><li><p><span>After analyzing the feature space of the style, </span><a href='https://docs.google.com/document/d/1rNVtQL071r6J8sj0mV0zDaNAnrczTJebBMsZwmm08to/edit?usp=sharing'><span>T-SNE plot</span></a><span> we decided that sharing the weights between the style encoders could help the network to extract informative features. Since the results were not affected by this ablation we kept it. (</span><a href='https://www.comet.ml/gcosne/munit-one-style/b42076bac551495d86bda1d1011e11bc?experiment-tab=images'><span>See Experiment</span></a><span>)</span></p></li><li><p><span>We shrink the architecture to use a single AutoEncoder and concluded that it was either longer to converge or that the transformation was harder to learn since the results were affected negatively. (</span><a href='https://www.comet.ml/gcosne/munit-uni-encoder/7cd0051ad112466190bb6d31f7291cc7'><span>See Experiment</span></a><span>)</span></p></li><li><p><span>Based on the fact that the flooding process is destructive and that there is no reason that the network could reconstruct the road from the flooded version, we implemented a weaker version of the Cycle Consistency Loss where the later is only computed on a specific region of the image. The specific region is defined by a binary mask of where we think the image should be altered. For example a flooded image mapped to a non-flooded house should only be altered in an area close by the one delimited by the water. (In practice there are bias intrinsic to the dataset such as the sky often being gray in a image of flood) (</span><a href='https://www.comet.ml/gcosne/munit-v2/3ee5653a971a459486edc45c12a0ae22?experiment-tab=images'><span>See Experiment</span></a><span>)</span></p></li><li><p><span>We trained a </span><a href='https://github.com/cc-ai/floods-gans/tree/master/flood-classifier'><span>classifier</span></a><span> to distinguish between flooded and non-flooded images (binary output) then use it when training MUNIT with a Loss on the generator indicating that fake flooded (resp non-flooded) images should be identified as flooded (resp non-flooded) by the classifier. It didn&#39;t improve the results we had, like if the flood classifier was a very bad discriminator that the generator could trick easily. (</span><a href='https://www.comet.ml/gcosne/munit-one-style-classifier/9bc7d50408ab4393a09acaadcda58e6a?experiment-tab=images'><span>See Experiment</span></a><span>)</span></p></li><li><p><span>To push the style encoder towards learning meaningful information, we investigated how to anonymise the representation of the content feature learned by MUNIT encoder. The idea behind is that if the content feature doesn&#39;t contains information about the domain it has been encoded, then the style would encode this information. We hence minimized the mutual information between the </span><em><span>content</span></em><span> feature and the source of the </span><em><span>content</span></em><span>. To do so we used a Domain-Classifier as in </span><a href='https://arxiv.org/abs/1802.09386'><span>Learning Anonymized Representations with Adversarial Neural Networks</span></a></p></li><li><p><span>We experiment playing with the training ratio of the Discriminator and the Generator. We empirically found that a factor 5 does improve slightly the convergence speed.</span></p></li></ul><p style="text-align:justify;"><b>Major Changes:</b> Introducing a Semantic Consistency Loss, we use <a href="https://github.com/cc-ai/floods-gans/tree/master/ground_segmentation">DeepLab v2</a> trained on cityscape to infer semantic label, and implemented an additional loss indicating to the generator that every fake image should keep the same Semantic as the source image before translation everywhere except a defined region where we think there should be an alteration. This modification dramatically improved our results. (<a href="https://www.comet.ml/gcosne/munit-v2-semantic-seg/68a929738a20413f885d2d684d05bd47?experiment-tab=images">See Experiment</a>) </p><p style="text-align:justify;">We also experimented with <a href="https://github.com/kazuto1011/deeplab-pytorch">DeepLab v2</a> trained on <a href="https://github.com/kazuto1011/deeplab-pytorch/blob/master/data/datasets/cocostuff/README.md">COCO-Stuff</a>. We thought this version would better suit our problem because it is able to identify water on the road but it turned out that (maybe because of the large number of classes) it didn't constrained much the network as with the previous version. We also tried to merge the classes from coco-stuff to only keep meta-classes that would be similar to cityscapes, it would allow us to keep a small number of classes and leverage the potential of identifying the water. (Impossible with Cityscape classes) (<a href="https://github.com/cc-ai/MUNIT/blob/feature/cocoStuff_merged_logits/README.md">See Results</a>).<h2><a name="how-to-leverage-simulated-data-" class="md-header-anchor"></a><span>How to leverage simulated data ?</span></h2><p style="text-align:justify;">We plan of using a simulated world built by Vahe Vardanyan with the Graphics Engine Unity to simulate different types of houses and streets under flood conditions to help our GAN understand where it should flood. </p>
<div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_synthetic.png" style="zoom:55%;" alt="{{ include.description }}" class="center"> 
  <figcaption><b>Non-flood Synthethic sample</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <b>Flood Synthetic sample</b>&nbsp;&nbsp;&nbsp; </figcaption> 
</figure>
</div><p style="text-align:justify;">One main advantage of using synthetic data is that theoretically we would have access to an unlimited amount of pairs. The principal difficulty lies in leveraging those pairs despite the existing discrepancy between the distribution of synthetic and real data. </p>
<div style="text-align: center">
<figure class="image"> 
  <img src="https://raw.githubusercontent.com/cc-ai/MUNIT/master/results/illustration_tsne.png" style="zoom:100%;" alt="{{ include.description }}" class="center"> 
  <figcaption></figcaption> 
</figure>
</div><p style="text-align:justify;">We can visualize the discrepancies between the differents domains with a T-SNE plot. Learning to flood natural images is equivalent to adapt samples from domain <img src="https://latex.codecogs.com/gif.latex?X_1" title="X_1"> to domain <img src="https://latex.codecogs.com/gif.latex?X_2" title="X_2"> and we would like to help the network learn this translation with an <em> easier task</em>: translating from <img src="https://latex.codecogs.com/gif.latex?X_{1\_Synthetic}" title="X_1"> to <img src="https://latex.codecogs.com/gif.latex?X_{2\_Synthetic}" title="X_1">. Indeed, probably because of its pairs, the gap separating the synthetic domains is smaller than for the real one. We also notice that some of the real data are mixed with the synthetic cluster, somehow a proof that the synthetic world is well imitating the real world.</p> 
<p style="text-align:justify;">We mix simulated data with their natural equivalent (synthetic flooded images with flooded images) at training time with an additional pixelwise reconstruction loss computed on the pixel that shouldn't be altered. </p>
<p style="text-align:justify;"><img src="https://latex.codecogs.com/gif.latex?\large&amp;space;\mathcal{L}_{synthetic}(x_1,x_{1\rightarrow&amp;space;2})&amp;space;=&amp;space;\left&amp;space;|$mask$\cdot(x_1$-$x_{1\rightarrow&amp;space;2})&amp;space;|&amp;space;\right&amp;space;|" title="\large \mathcal{L}_{synthetic}(x_1,x_{1\rightarrow 2}) = \left |$mask$\cdot(x_1$-$x_{1\rightarrow 2}) | \right |"> </p>
<p style="text-align:justify;">Where <b>mask</b>  <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&amp;space;\large&amp;space;$mask$&amp;space;=&amp;space;(x_1==x_2)" title="\large $mask$ = (x_1==x_2)"> correspond to the region of pixels where <img src="https://latex.codecogs.com/gif.latex?x_1" title="x_1"> and <img src="https://latex.codecogs.com/gif.latex?x_2" title="x_1"> are paired, in our case, where there is no water.</p><h2><a name="evaluating-the-realism-of-our-results" class="md-header-anchor"></a><span>Evaluating the realism of our results </span></h2><p style="text-align:justify;">We synthesized our attempt to establish an automated evaluation metric to quantify fake Image  realism in the following <a href="https://arxiv.org/abs/1910.10143">paper</a> . Our work consisted in adapting several existing metrics (<a href="https://arxiv.org/abs/1801.01973">IS</a>,<a href="https://arxiv.org/abs/1706.08500">FID</a>, <a href="https://arxiv.org/abs/1801.01401">KID</a>..) and assessing them against gold standard human evaluation: <a href="https://arxiv.org/abs/1904.01121">HYPE</a>. While insufficient alone to establish a human-correlated automatic evaluation metric, we believe this work begins to bridge the gap between human and automated generative evaluation procedures.</p><h2><a name="data-mining-and-annotation" class="md-header-anchor"></a><span>Data Mining And Annotation: </span></h2><p><span>We set a goal of recovering about 1000 images in each domain meeting a number of criteria.</span></p><p><strong><em><span>Flooded Houses</span></em></strong><span>: images should present a part of a single house or building visible and the street partially or fully covered by water.</span></p><p><span>These images have been gathered using the results of different Google Image queries focusing on North-American suburban type of houses. </span></p><p><strong><em><span>Non-flooded houses are a mix of several types of images:</span></em></strong><span> </span></p><ul><li><span>Single houses with grass gathered manually from the Web.</span></li><li><span>Street-level imagery extracted from Google StreetView API.</span></li><li><span>Diverse street-level imagery covering a variety of weather conditions, seasons, times of day  and viewpoints taken from a publicly available dataset.</span></li></ul><p><span>Motivated by the idea that it would be easier to perform Image To Image Translation if our GAN had an idea of what the concepts of </span><strong><span>Ground</span></strong><span> and </span><strong><span>Water</span></strong><span> are, we increased the knowledge we had on the dataset by annotating pixels corresponding to </span><strong><span>Water</span></strong><span> in the Flooded Houses images and those corresponding to the </span><strong><span>Ground</span></strong><span> in Non-flooded houses images.</span></p><ul><li><strong><span>70%</span></strong><span> of the Flooded Houses were annotated using a Semantic Segmentation Network, namely  </span><a href='https://github.com/cc-ai/deeplab-pytorch'><span>DeepLab v2 trained on COCO-stuffs-164k dataset</span></a><span> and merging some labels to output a binary mask of water.</span></li><li><strong><span>30%</span></strong><span> of them have been manually annotated using </span><a href='https://labelbox.com/'><span>LabelBox</span></a><span>.</span></li><li><strong><span>100%</span></strong><span> of the Non Flooded Houses were automatically segmented using </span><a href='https://github.com/cc-ai/floods-gans/blob/master/ground_segmentation/README.md'><span>DeepLab trained on CityScapes</span></a><span>.</span></li></ul></div>

<div  id='write'  class = 'is-node'><h2><a name="simulated-data" class="md-header-anchor"></a><span>Simulated data</span></h2><p><span>To create our simulated world, we used the Unity 3D game engine (version 2018.2.21f1). We created different types of buildings (skyscrapers, individual houses, industrial buildings) in the virtual world, combined with attributes of urban and rural environments: roads, trees, cars, mountains, vegetation, etc.</span></p><p><span>For each shot captured in the simulated world, we extract: </span></p><ul><li><span>Original image (non-flooded)</span></li><li><span>Flooded image </span></li><li><span>Binary masks of the flood location</span></li><li><span>Depth image</span></li><li><span>Semantic segmentation image</span></li><li><span>Json file with camera parameters </span></li></ul><p><a href=' https://github.com/cc-ai/height_estimation/blob/master/simulated_world/README.md '><span>See a more detailed description of each of these</span></a></p><p><span>We currently have 11k such captures. Note that each scene has been captured multiple times varying slightly the camera height, pitch, and position. </span></p><h1><a name="height-estimation" class="md-header-anchor"></a><span>Height estimation </span></h1><p><span>One of the challenges is to match the level of the generated flood to climate predictions, or at least to a plausible water level. In the aforementioned approaches, no notion of the geometry of the scene is included. We propose to introduce height information in our model in order to generate and respect the geometry of the scene. </span></p><p><span>Images obtained using masks based on semantic segmentation (ground -&gt; water)  in the weak cycle consistency constraint present realistic water texture, but lack physical realisticness, with water covering only the pixels corresponding to the ground and circumventing the wheels of cars, for example.</span></p><p><span>There are two main ways we can think of to include height information in our model and condition the GAN on water height: </span></p><ul><li><span>At training time : compute binary masks of the areas that should be flooded and include them for computing the weak cycle consistency loss. Note that this might imply discretizing the flood levels (e.g. have 3 levels : low, medium, high - that would correspond to certain metric levels). </span></li><li><span>At inference time : condition the output on a binary mask of the area to flood (no discretization of flood levels needed) - This would require to change the MUNIT architecture/ or to use another architecture (e.g. it is be possible to put masks at inference time as the semantic layout in </span><a href=' https://arxiv.org/pdf/1903.07291.pdf '><span>SPADE</span></a><span>)</span></li></ul><p><span>Here we present the two types of approaches that we have tried so far to estimate height in street scene images : &quot;geometrical approaches&quot; which consist in recovering the 3D metric geometry of the scene, and  &quot;end-to-end approaches&quot; which consist in predicting height from an image input.  The problem of estimating height from single view image has not been much explored in the literature, and there is no dataset of street scenes with associated height maps directly available . </span></p><h2><a name="geometrical-approaches" class="md-header-anchor"></a><span>Geometrical approaches</span></h2><p><span>There are three steps in the geometric approaches to recover height: </span></p><p><span>1) Recover the 3D geometry of the scene (not metric)</span></p><p><span>2) Match relative coordinates to metric system in some way</span></p><p><span>3) Create binary masks of the areas to be flooded using a metric threshold </span></p><h3><a name="step-1--recover-the-3d-geometry-of-the-scene" class="md-header-anchor"></a><strong><span>Step 1 : Recover the 3D geometry of the scene</span></strong><span> </span></h3><p><span>The </span><a href=' https://github.com/cc-ai/height_estimation/blob/master/pinhole/README.md '><span>pinhole camera model</span></a><span> can be applied to recover 3D coordinates of each pixel of the image using depth maps and camera parameters. </span>
<span>Camera parameters considered in our case are the field of view and the pitch (roll and yaw are uassumed to be ~0), and we assume that pixels are square and that  the optical center of the camera is in the middle of the image.</span><br/><span>Pseudo relative-depth information is obtained using  </span><a href=' http://www.cs.cornell.edu/projects/megadepth/ '><span>MegaDepth</span></a><span>. </span></p><p><span>We can then derive the 3D geometry of the scene, and assuming that the water level of the floods corresponds to horizontal planes, we can extract the masks by taking all pixels with height values below a certain fixed threshold. </span></p><p><strong><span>Limitations </span></strong><span>: </span></p><ul><li><span>MegaDepth does not predict metric depth.</span></li><li><span>On our images, we noticed MegaDepth can be unreliable when there is a strong perspective, or for thin objects  (e.g. poles)</span></li><li><span>Choosing a threshold on relative coordinates does not guarantee that the flood level will be consistently plausible on all the images, and correspond to physical reality. </span></li><li><span>While camera parameters can be specified when fetching data through the GSV API, they are unknown for the other real images we gathered to train our flood model. </span></li></ul><h3><a name="step-2-match-relative-coordinates-to-metric-system" class="md-header-anchor"></a><strong><span>Step 2: Match relative coordinates to metric system</span></strong></h3><h4><a name="1--using-reference-objects-in-the-image" class="md-header-anchor"></a><span>1- Using reference objects in the image</span></h4><p><span> </span><img src="https://github.com/cc-ai/height_estimation/blob/master/src/docs/pipelinev1.JPG?raw=true" referrerpolicy="no-referrer" alt="alt text"></p><p><span>One way of  recovering metric scaling is to find reference objects that are often present in the dataset, and for which we know the dimensions : vehicles, pedestrians. </span>
<span>We used the </span><a href=' https://arxiv.org/abs/1612.00496 '><span>3D Bounding Box detection model</span></a><span> of Mousavian et al. to to detect cars, trucks and pedestrians. There are two steps in this method: first, perform 2D bounding box detection, and then use CNNs to regress the 3D parameters and dimensions. </span>
<span>We used a pretrained model on </span><a href='http://www.cvlibs.net/datasets/kitti/'><span>KITTI dataset</span></a><span>, hence the choice of the classes we kept as reference objects. </span>
<span>When applying this model to our street scene images, we noticed that the objects were sometimes misclassified, especially if only part of a vehicle was visible (this model of 3D bounding box does not extend outside the frame of the picture).  Hence, we decided to perform semantic segmentation on the whole input image, and checked that the labels from the segmentation were the same as the detections. We used </span><a href=' https://github.com/cc-ai/floods-gans/tree/master/ground_segmentation'><span>this Deeplab segmentation model</span></a><span> which was trained on Cityscapes. </span>
<span>Because there were discrepancies between KITTI labels and Cityscapes labels, we only kept the classes truck, car and pedestrian.  </span></p><p><span>We match the relative coordinates of the delimiters of the vertical edges of the bounding box to the metric height of the detected objects. We assume the objects are on the ground (which is almost sure to be the case) and match the bottom of the vertical edges to zero level (ground). </span></p><p><span>If no reference object is found, we output a mask that corresponds to the ground class in the semantic segmentation.</span></p><p><strong><span>Limitations</span></strong></p><p><span>Reference objects are not always present in the images. </span>
<span>And when they are, if the object is far away, the scaling from relative height to metric might be very unreliable. This method seems to work well when there are multiple objects, and when they are more or less in plane facing the camera (at similar depth ). </span></p><h4><a name="2--using-the-ground-as-reference" class="md-header-anchor"></a><span>2- Using the ground as reference</span></h4><p><span>In this method, we do not rely on reference objects but rather on geometry.  We assume that camera height is known (this is reasonable for street-level images extracted from known platforms) and that the ground is flat. </span></p><p><span>We assume that camera height is known (this is reasonable for street-level images extracted from known platforms).</span></p><p><span> </span><img src="https://github.com/cc-ai/height_estimation/blob/master/src/docs/pipelinev2.JPG?raw=True" referrerpolicy="no-referrer" alt="alt text"></p><p><span>We scale the relative coordinates to metric using ground as a reference. TO do so, we take the midle column of the image (right in the camera axis) and take two pixels that were segmented as belonging to the ground.  We consider that both should be at height level 0 (assumption that ground is flat)</span></p><p><img src="https://github.com/cc-ai/height_estimation/blob/master/src/docs/geometry.png?raw=true" referrerpolicy="no-referrer" alt="alt text"></p><p><span>In this graphic, </span><img src = "https://latex.codecogs.com/gif.latex?$h_c$"><span> is the height of the camera, and </span><img src = "https://latex.codecogs.com/gif.latex?$\alpha$"><span> is half of the vertical  field of view. </span></p><p><span>The matching to metric scaling we make is as follows (we refer to the above graphic): </span></p><ul><li><p><span>take 2 points segmented as ground in the image (red dots on the projection plane)</span></p></li><li><p><span>the angles </span><img src = "https://latex.codecogs.com/gif.latex?$\beta$"><span> and </span><img src = "https://latex.codecogs.com/gif.latex?$\gamma$"><span> can be obtained using the y (vertical axis) pixel location in the image. </span>
<span>Let H be the height of the image, and Y the distance in pixels from the middle of the image to the red pixel corresponding to the angle </span><img src = "https://latex.codecogs.com/gif.latex?$\beta$"></p><p><img src = "https://latex.codecogs.com/gif.latex?\dfrac{tan&space;\beta}{tan&space;\alpha}&space;=&space;\dfrac{Y}{H/2}"></p><p><span>From there we can compute the yellow distance in meters:</span></p><p><img src = "https://latex.codecogs.com/gif.latex?d=&space;\dfrac{h_c}{tan&space;\beta&space;}&space;-&space;\dfrac{h_c}{tan&space;\gamma&space;}$"></p></li><li><p><span>We match this distance to the difference of relative depth to get the scaling from relative units to meters.</span></p></li></ul><p><strong><span>Limitations</span></strong></p><p><span>The assumption that the ground is flat does not always hold. While in the images we tested our approach on, there were always pixels labelled as ground in the center vertical column of pixels of the image, it could be that because of vehicles, vegetation or others, there would be none. </span>
<span>This also relies on two pixels only, and there would be some additional work to do to determine which ground pixels to choose (they shouldn&#39;t be too close in the front of the image because sometimes, there is warping and the ground is &quot;stretched &quot; in front of the camera etc.)</span></p><h4><a name="3--create-binary-masks-using-a-specified-theshold" class="md-header-anchor"></a><span>3- Create binary masks using a specified theshold </span></h4><p><span>Once we have a correspondence between relative and metric coordinates, we can take all the pixels with height less than a threshold specified in meters and generate binary masks of these. </span></p><p><span>We added the option of merging the mask corresponding to ground segmentation and the mask obtained with the geometrical approach. The underlying idea is that sometimes depth maps obtained using MegaDepth can be unreliable for elements that are far away, but we would still like to flood ground. This holds only if the difference in heights of the ground is not too big ( no big slope), to satisfy the condition that the flood surface should be a horizontal plane. </span></p><h2><a name="end-to-end-height-estimator" class="md-header-anchor"></a><span>End-to-End height estimator </span></h2><p><span>One of the challenges in height estimation is that there is no ground truth dataset of height maps of street-level images. But ground truth height maps can be obtained from simulated data. Beyond building a ground truth dataset, we propose to leverage data from our simulator and train a height estimation model from single-view images. Indeed, images of houses flooded to any chosen height can be generated in the simulator. While methods for single image depth estimation have been investigated for many years, we have found no work in height estimation from street view scenes images. However, these two problems have some similarities, so we took inspiration from depths estimators to build our height estimator.</span></p><p><span>Following the success of  </span><a href=' http://www.cs.cornell.edu/projects/megadepth/ '><span>MegaDepth</span></a><span> on the task of single-view depth estimation, we chose to train an </span><a href=' https://arxiv.org/pdf/1604.03901.pdf '><span>hourglass network</span></a><span> to predict metric height maps from input images of street-level images.</span></p><p><span> </span><img src="https://github.com/cc-ai/height_estimation/blob/master/hourglass/docs/hourglass.JPG?raw=true" referrerpolicy="no-referrer" alt="hourglass"><span> </span></p><p><em><span>Architecture of the hourglass network (Image modified from </span><a href=' https://arxiv.org/pdf/1604.03901.pdf '><span>here</span></a><span>).</span></em><span> </span>
<span>Each block is a modified inception module, except blocks H which are Conv 3x3.</span></p><p><span>For our first model, we chose an L2 loss with a mask on the sky. </span></p><p><span>We tried this approach on  data from our simulator and data obtained with the </span><a href=' http://carla.org/ '><span>CARLA</span></a><span> simulator separately. One of the main next steps would be to try this approach on real images. </span>
<a href=' https://github.com/cc-ai/height_estimation/blob/master/hourglass/README.md '><span>You can find more details about the model here</span></a></p><p>&nbsp;</p></div>
</body>
 </body>
</html>
